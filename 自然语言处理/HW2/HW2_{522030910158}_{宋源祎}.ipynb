{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作业二：实现Word2Vec的连续词袋模型\n",
    "\n",
    "姓名：宋源祎\n",
    "\n",
    "学号：522030910158\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本次作业将使用PyTorch构建CBOW模型。Pytorch是当下最主流的深度学习框架，在大作业中我们将继续使用torch完成语言模型的载入、训练、推理等操作。希望同学们能够通过这次作业对torch的张量操作以及常用函数有一个基础的理解，以便应用于之后的作业以及其他的深度学习实践当中。\n",
    "\n",
    "依据计算平台的不同，PyTorch提供了多种版本可供安装。本次作业我们只需要使用CPU版本，可以通过通过`pip install torch`直接安装。\n",
    "\n",
    "> 关于GPU版本的安装可以参见[官网](https://pytorch.org/get-started/locally/)。对于本次作业，由于模型参数太小，使用GPU进行运算与CPU相比并无优势，**请不要使用GPU训练模型。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T06:35:38.715014900Z",
     "start_time": "2023-11-05T06:35:35.552612900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple/\n",
      "Requirement already satisfied: torch in c:\\users\\86189\\appdata\\roaming\\python\\python310\\site-packages (2.0.1+cu117)\n",
      "Requirement already satisfied: tqdm in c:\\users\\86189\\.conda\\envs\\pytorch\\lib\\site-packages (4.66.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\86189\\.conda\\envs\\pytorch\\lib\\site-packages (from torch) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\86189\\.conda\\envs\\pytorch\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\86189\\.conda\\envs\\pytorch\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\86189\\.conda\\envs\\pytorch\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\86189\\.conda\\envs\\pytorch\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\86189\\.conda\\envs\\pytorch\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\86189\\.conda\\envs\\pytorch\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\86189\\.conda\\envs\\pytorch\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要Python版本大于等于3.6，并检查是否已安装所依赖的第三方库。（若没有安装可以执行上面的代码块）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries are satisfied.\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import sys\n",
    "\n",
    "assert sys.version_info[0] == 3\n",
    "assert sys.version_info[1] >= 6\n",
    "\n",
    "requirements = [\"torch\", \"tqdm\"]\n",
    "_OK = True\n",
    "\n",
    "for name in requirements:\n",
    "    try:\n",
    "        importlib.import_module(name)\n",
    "    except ImportError:\n",
    "        print(f\"Require: {name}\")\n",
    "        _OK = False\n",
    "\n",
    "if not _OK:\n",
    "    exit(-1)\n",
    "else:\n",
    "    print(\"All libraries are satisfied.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 辅助代码\n",
    "\n",
    "该部分包含：用于给句子分词的分词器`tokenizer`、用于构造数据的数据集类`Dataset`和用于构建词表的词表类`Vocab`。\n",
    "\n",
    "> 注：该部分无需实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分词器\n",
    "\n",
    "该分词器会：\n",
    "1. 将所有字母转为小写；\n",
    "2. 将句子分为连续的字母序列（word）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', 's', 'useful']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import List, Tuple\n",
    "\n",
    "def tokenizer(line: str) -> List[str]:\n",
    "    line = line.lower()  # Lowercasing\n",
    "    tokens = list(filter(lambda x: len(x) > 0, re.split(r\"\\W\", line))) # Splitting\n",
    "    return tokens\n",
    "\n",
    "print(tokenizer(\"It's  useful. \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集类\n",
    "\n",
    "语料数据集类`CorpusDataset`读取`corpus`中的行，并依据设定的窗口长度`window_size`解析返回`(context, target)`元组。\n",
    "\n",
    "假如一个句子序列为`a b c d e`，且此时`window_size=2`，`CorpusDataset`会返回：\n",
    "\n",
    "```\n",
    "([b, c], a)\n",
    "([a, c, d], b)\n",
    "([a, b, d, e], c)\n",
    "([b, c, e], d)\n",
    "([c, d], e)\n",
    "```\n",
    "\n",
    "> 该`CorpusDataset`类继承自torch提供的数据集类`Dataset`。torch对该类提供了多种工具函数，配合`DataLoader`可以便捷地完成批次加载、数据打乱等数据集处理工作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CorpusDataset(Dataset):\n",
    "    def __init__(self, corpus_path: str, window_size: int) -> None:\n",
    "        \"\"\"\n",
    "        :param corpus: 语料路径\n",
    "        :param window_size: 窗口长度\n",
    "        \"\"\"\n",
    "        self.corpus_path = corpus_path # 语料路径\n",
    "        self.window_size = window_size # 窗口长度\n",
    "\n",
    "        self.data = self._load_data()\n",
    "\n",
    "    def _load_data(self) -> List:\n",
    "        # 读取语料，返回元组\n",
    "        data = []\n",
    "        with open(self.corpus_path, encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                tokens = tokenizer(line)  # 分词列表\n",
    "                if len(tokens) <= 1:\n",
    "                    # 没有词\n",
    "                    continue\n",
    "                for i, target in enumerate(tokens):\n",
    "                    # 每一次词作为中心词，i为中心词索引\n",
    "                    left_context = tokens[max(0, i - self.window_size): i] # 左边提取窗口长度个，索引等于中心词左边词的个数\n",
    "                    right_context = tokens[i + 1: i + 1 + self.window_size] # 右边提取窗口长度个，不要大于总数（min）\n",
    "                    context = left_context + right_context \n",
    "                    data.append((context, target)) # append 元组\n",
    "        return data\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        # 返回语料数据集长度\n",
    "        return len(self.data) \n",
    "\n",
    "    def __getitem__(self, idx) -> Tuple[List[str], str]:\n",
    "        # 按照索引取值\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "(['want', 'to', 'go'], 'i')\n",
      "(['i', 'to', 'go', 'home'], 'want')\n",
      "(['i', 'want', 'go', 'home'], 'to')\n",
      "(['i', 'want', 'to', 'home'], 'go')\n"
     ]
    }
   ],
   "source": [
    "debug_dataset = CorpusDataset(\"./data/debug2.txt\", window_size=3) # 返回一个数据集对象，每个元素是上下文+中心词元组\n",
    "print(len(debug_dataset)) # 语料数据集长度\n",
    "\n",
    "for i, pair in enumerate(iter(debug_dataset)):\n",
    "    print(pair) # 打印前三个元素\n",
    "    if i >= 3:\n",
    "        break\n",
    "\n",
    "del debug_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词表类\n",
    "\n",
    "`Vocab`可以用`token_to_idx`把token(str)映射为索引(int)，也可以用`idx_to_token`找到索引对应的token。\n",
    "\n",
    "实例化`Vocab`有两种方法：\n",
    "1. 读取`corpus`构建词表。\n",
    "2. 通过调用`Vocab.load_vocab`，可以从已训练的词表中构建`Vocab`实例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from typing import Dict\n",
    "\n",
    "class Vocab:\n",
    "    VOCAB_FILE = \"vocab.txt\"\n",
    "    UNK = \"<unk>\"\n",
    "\n",
    "    def __init__(self, corpus: str=None, max_vocab_size: int=-1):\n",
    "        \"\"\"\n",
    "        :param corpus:         语料文件路径\n",
    "        :param max_vocab_size: 最大词表数量，-1表示不做任何限制\n",
    "        \"\"\"\n",
    "        self._token_to_idx: Dict[str, int] = {}  # 词到索引的映射，字典\n",
    "        self.token_freq: List[Tuple[str, int]] = [] # 词频列表，元组(词, 词频)，词表\n",
    "\n",
    "        if corpus is not None:\n",
    "            # 读取语料，构建词表\n",
    "            self.build_vocab(corpus, max_vocab_size)\n",
    "\n",
    "    def build_vocab(self, corpus: str, max_vocab_size: int=-1):\n",
    "        \"\"\" 统计词频，并保留高频词 \"\"\"\n",
    "        counter = Counter()  # Counter是字典的子类，用于计数，key是元素，value是个数\n",
    "        with open(corpus, encoding=\"utf-8\") as f:\n",
    "            # 按照路径打开语料文件\n",
    "            for line in f:\n",
    "                tokens = tokenizer(line)  # 每一句话都转化为列表\n",
    "                counter.update(tokens)  # 对于每个词，更新计数\n",
    "\n",
    "        print(f\"总Token数: {sum(counter.values())}\")  \n",
    "        # 打印总词数（词频和，没限制大小）\n",
    "\n",
    "        # 将找到的词按照词频从高到低排序\n",
    "        self.token_freq = [(self.UNK, 1)] + sorted(counter.items(), key=lambda x: x[1], reverse=True) # 降序排列，第一个是UNK\n",
    "        if max_vocab_size > 0:\n",
    "            self.token_freq = self.token_freq[:max_vocab_size]  # 限制词表大小max_vocab_size\n",
    "\n",
    "        print(f\"词表大小: {len(self.token_freq)}\")\n",
    "        print(f\"限制大小后的词表总Token数: {sum(self.token_freq[i][1] for i in range(len(self.token_freq)))}\")  # 多的一个是<UNK>\n",
    "\n",
    "        for i, (token, _freq) in enumerate(self.token_freq):\n",
    "            self._token_to_idx[token] = i # 词到索引的映射（key->i）\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_freq) # 词表长度\n",
    "\n",
    "    def __contains__(self, token: str):\n",
    "        return token in self._token_to_idx  # 判断词是否在词表中\n",
    "\n",
    "    def token_to_idx(self, token: str, warn: bool = False) -> int:\n",
    "        \"\"\" 将token映射至索引 \"\"\"  # 返回token的索引\n",
    "        token = token.lower()  # 转化为小写\n",
    "        if token not in self._token_to_idx:\n",
    "            # 如果不在词典中，标记为UNK\n",
    "            if warn:\n",
    "                warnings.warn(f\"{token} => {self.UNK}\")\n",
    "            token = self.UNK  # 不在词典中，标记为UNK\n",
    "        return self._token_to_idx[token]\n",
    "\n",
    "    def idx_to_token(self, idx: int) -> str:\n",
    "        \"\"\" 将索引映射至token \"\"\"  # 给索引，返回token\n",
    "        assert 0 <= idx < len(self), f\"Index {idx} out of vocab size {len(self)}\"\n",
    "        return self.token_freq[idx][0] # 0是token，1是词频\n",
    "\n",
    "    def save_vocab(self, path: str):\n",
    "        \"\"\" 保存词表至文件路径path \"\"\"\n",
    "        with open(os.path.join(path, self.VOCAB_FILE), \"w\", encoding=\"utf-8\") as f:\n",
    "            lines = [f\"{token} {freq}\" for token, freq in self.token_freq]\n",
    "            f.write(\"\\n\".join(lines))\n",
    "\n",
    "    @classmethod\n",
    "    def load_vocab(cls, path: str):\n",
    "        vocab = cls() # 创建一个词表对象，自己\n",
    "\n",
    "        with open(os.path.join(path, cls.VOCAB_FILE), encoding=\"utf-8\") as f:\n",
    "            # 读取词表文件\n",
    "            lines = f.read().split(\"\\n\")\n",
    "\n",
    "        for i, line in enumerate(lines):\n",
    "            # 转化为本地词表\n",
    "            token, freq = line.split()\n",
    "            vocab.token_freq.append((token, int(freq)))\n",
    "            vocab._token_to_idx[token] = i\n",
    "\n",
    "        return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总Token数: 50\n",
      "词表大小: 21\n",
      "限制大小后的词表总Token数: 51\n",
      "[('<unk>', 1), ('want', 6), ('to', 6), ('go', 4), ('i', 3), ('home', 3), ('play', 3), ('like', 3), ('eating', 3), ('he', 3), ('she', 3), ('it', 2), ('is', 2), ('we', 2), ('useful', 1), ('awful', 1), ('can', 1), ('read', 1), ('books', 1), ('will', 1), ('now', 1)]\n"
     ]
    }
   ],
   "source": [
    "debug_vocab = Vocab(\"./data/debug2.txt\")  # 创建一个词表对象，路径为句子文件\n",
    "print(debug_vocab.token_freq)  # 打印词表  限制大小后多的一个是<UNK>\n",
    "del debug_vocab  # 删除词表对象"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec实现\n",
    "\n",
    "本节将实现Word2Vec的CBOW模型，为了便于实现，本实验不引入`Hierarchical Softmax`和`Negative Sampling`等加速技巧，若同学们对这些技术感兴趣，可参考：[word2vec Parameter Learning Explained](https://arxiv.org/pdf/1411.2738.pdf)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 实现one-hot向量构建函数\n",
    "\n",
    "需求：指定词向量的维度和需要置1的索引，返回`torch.Tensor`张量格式的one-hot行向量。\n",
    "\n",
    "请手动操作张量实现该需求， **不要直接使用库中已有的`torch.nn.functional.one_hot`函数，否则不得分！** 你可以在实现后与库函数的结果相比对来验证正确性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(idx: int, dim: int) -> torch.Tensor:\n",
    "    # [1] TODO: 实现one-hot函数【1分】\n",
    "    t = torch.zeros(dim, dtype=torch.int64)\n",
    "    t[idx] = 1\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "参考值：tensor([0, 1, 0, 0])\n",
      "测试值：tensor([0, 1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "src = one_hot(1, 4)\n",
    "ref = F.one_hot(torch.tensor(1), num_classes=4)\n",
    "# print(ref.dtype)\n",
    "print(f\"参考值：{ref}\")\n",
    "print(f\"测试值：{src}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 实现softmax函数\n",
    "请手动操作张量，实现softmax函数。**直接使用torch的softmax方法不得分！**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x: torch.Tensor) -> torch.Tensor:\n",
    "    # [2] TODO: 实现softmax函数【2分】\n",
    "    max = torch.max(x)\n",
    "    t = torch.exp(x-max)\n",
    "    sum = torch.sum(t)\n",
    "    return t / sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "参考值：tensor([7.8013e-05, 2.1206e-04, 5.7645e-04, 1.5669e-03, 4.2594e-03, 1.1578e-02,\n",
      "        3.1473e-02, 8.5552e-02, 2.3255e-01, 6.3215e-01])\n",
      "测试值：tensor([7.8013e-05, 2.1206e-04, 5.7645e-04, 1.5669e-03, 4.2594e-03, 1.1578e-02,\n",
      "        3.1473e-02, 8.5552e-02, 2.3255e-01, 6.3215e-01])\n"
     ]
    }
   ],
   "source": [
    "src = softmax(torch.arange(10).float())\n",
    "ref = F.softmax(torch.arange(10).float(), dim=0)\n",
    "print(f\"参考值：{ref}\")\n",
    "print(f\"测试值：{src}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 实现CBOW类并训练模型\n",
    "\n",
    "推荐按照TODO描述的步骤以及限定的代码块区域来实现（预计15行代码），也可在保证结果正确的前提下按照自己的思路来实现。请手动操作张量实现反向传播与模型训练，**直接使用loss.backward()、optimizer等torch内置方法不得分！**\n",
    "\n",
    "> 建议利用torch提供的张量操作（点积、外积、矩阵乘等）替代python的循环，高效处理数据。\n",
    "\n",
    "> `torch.nn.Module`是torch中神经网络模型的基类，大多数模型的定义都继承于此。其中的`forward`函数相当于`__call__`方法，一般用于处理模型的前向传播步骤。因此如果你定义了一个实例`cbow = CBOW()`，你可以直接用`cbow(input)`来调用它的`forward`函数并获得模型输出。\n",
    "\n",
    "> 一般来说，模型接受的输入往往是一个批次（batch）；本次作业为实现方便起见不使用batch，只需考虑单条输入的前向与反向传播即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "class CBOW(torch.nn.Module):\n",
    "    def __init__(self, vocab: Vocab, vector_dim: int):\n",
    "        super().__init__()\n",
    "        self.vocab = vocab  # 词表\n",
    "        self.vector_dim = vector_dim # 词向量维度\n",
    "\n",
    "        # 自回归模型，U_proj是输入层到隐层的权重，V_proj是隐层到输出层的权重\n",
    "        self.U_proj = torch.nn.Linear(len(self.vocab), vector_dim, bias=False)  # 词表 -> 隐层词向量, D * N\n",
    "        self.V_proj = torch.nn.Linear(vector_dim, len(self.vocab), bias=False)  # 隐层词向量 -> 词表, N * D\n",
    "        torch.nn.init.uniform_(self.U_proj.weight, -1, 1)\n",
    "        torch.nn.init.uniform_(self.V_proj.weight, -1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h, o, y = None, None, None\n",
    "        # [3] TODO: 实现前向传播逻辑【3分】 ==========================>>>\n",
    "        # 使用之前定义的softmax函数完成输出概率的归一化\n",
    "        # 注意返回中间结果，以便于在训练时反向传播使用\n",
    "        x = x/torch.sum(x)\n",
    "        h = self.U_proj(x)\n",
    "        o = self.V_proj(h)\n",
    "        y = softmax(o)\n",
    "        # [3] <<<======================= END ==========================\n",
    "        return y, (h, o)\n",
    "\n",
    "    def train(self, corpus: str, window_size: int, train_epoch: int, learning_rate: float=1e-1, save_path: str = None):\n",
    "        dataset = CorpusDataset(corpus, window_size) # 创建数据集对象，每个元素是上下文+中心词元组\n",
    "        start_time = time.time()\n",
    "\n",
    "        for epoch in range(1, train_epoch + 1):\n",
    "            avg_loss = self.train_one_epoch(epoch, dataset, learning_rate)\n",
    "            if save_path is not None:\n",
    "                self.save_model(save_path)\n",
    "\n",
    "        end_time = time.time()\n",
    "        print(f\"总耗时 {end_time - start_time:.2f}s\")\n",
    "\n",
    "    def train_one_epoch(self, epoch: int, dataset: CorpusDataset, learning_rate: float) -> float:\n",
    "        steps, total_loss = 0, 0.0\n",
    "\n",
    "        with tqdm(dataset, desc=f\"Epoch {epoch}\") as pbar:\n",
    "            for sample in pbar:\n",
    "                context_tokens, target_token = sample\n",
    "                loss = self.train_one_step(context_tokens, target_token, learning_rate)\n",
    "                total_loss += loss\n",
    "                steps += 1\n",
    "                if steps % 10 == 0:\n",
    "                    pbar.set_postfix({\"Avg. loss\": f\"{total_loss / steps:.4f}\"})\n",
    "\n",
    "        return total_loss / steps\n",
    "\n",
    "    def train_one_step(self, context_tokens: List[str], target_token: str, learning_rate: float, debug: bool=False) -> float:\n",
    "        \"\"\"\n",
    "        :param context_tokens:  目标词周围的词\n",
    "        :param target_token:    目标词\n",
    "        :param learning_rate:   学习率\n",
    "        :return:    loss值 (标量)\n",
    "        \"\"\"\n",
    "        #print(context_tokens, target_token)\n",
    "        context, target = None, None\n",
    "        # [4] TODO: 使用one_hot函数，构建输入与输出的0-1向量【2分】 ===>>>\n",
    "        # indices = [self.vocab.token_to_idx(context_token) for context_token in context_tokens]\n",
    "        # print(indices)\n",
    "        one_hot_encoding = [\n",
    "            one_hot(self.vocab.token_to_idx(context_token), len(self.vocab))\n",
    "            for context_token in context_tokens\n",
    "        ]\n",
    "        # print(one_hot_encoding)\n",
    "        context = torch.stack(one_hot_encoding)\n",
    "        # print(context)\n",
    "        context = torch.sum(context, dim=0).float()\n",
    "        # print(context)\n",
    "        target = one_hot(self.vocab.token_to_idx(target_token), len(self.vocab)).float()\n",
    "        # [4] <<<======================= END ==========================\n",
    "\n",
    "        pred, (h, o) = self.forward(context)\n",
    "        # pred是预测值，分布形式；target是真实值，one-hot形式\n",
    "        # h是隐层输出，o是输出层输出\n",
    "\n",
    "        loss = None\n",
    "        # [5] TODO: 计算交叉熵损失loss【1分】 ========================>>>\n",
    "        loss = -torch.sum(target * torch.log(pred))\n",
    "        # [5] <<<======================= END ==========================\n",
    "\n",
    "        dV_proj, dU_proj = None, None\n",
    "        # [6] TODO: 计算U与V的梯度【3分】 ============================>>>\n",
    "        # TODO:预测值减去单位矩阵\n",
    "        # dE/du_jk=∑i(y_i*v_ij*x_k)-v_ij*x_k(i=target)\n",
    "        y = pred-target  # (1, N)\n",
    "        du_tmp = torch.matmul(y, self.V_proj.weight) # (1, N) * (N, D) = (1, D)\n",
    "        dU_proj = torch.outer(context, du_tmp).T/len(context_tokens)  # (N, 1) * (1, D) = (N, D) trans (D, N)\n",
    "        # 上述context是一个稀疏向量，因为速度很慢，所以在同学建议下尝试使用for循环处理，没有效果，于是使用原方式\n",
    "        # for context_token in context_tokens:\n",
    "        #     idx = self.vocab.token_to_idx(context_token)\n",
    "        #     dU_proj[:, idx] = du_tmp / len(context_tokens)\n",
    "        # dE/dv_ij=∑i(y_i*h_j)-h_j(i=target)\n",
    "        dV_proj = torch.outer(y, h)  # (N, 1) * (1, D) = (N, D)\n",
    "        # print(dV_proj.shape, dU_proj.shape)\n",
    "        # [6] <<<======================= END ==========================\n",
    "\n",
    "        # [7] TODO: 更新U与V的参数【2分】 ============================>>>\n",
    "        # self.U_proj.weight.data -= learning_rate * dU_proj\n",
    "        # self.V_proj.weight.data -= learning_rate * dV_proj\n",
    "        self.U_proj.weight = torch.nn.Parameter(self.U_proj.weight -\n",
    "                                                learning_rate * dU_proj)\n",
    "        self.V_proj.weight = torch.nn.Parameter(self.V_proj.weight -\n",
    "                                                learning_rate * dV_proj)\n",
    "        # [7] <<<======================= END ==========================\n",
    "\n",
    "        if debug:\n",
    "            print(f\"Loss: {loss.item()}\")\n",
    "            print(f\"Gradient of U_proj:\\n{dU_proj.detach().T}\")\n",
    "            print(f\"Gradient of V_proj:\\n{dV_proj.detach().T}\")\n",
    "\n",
    "        # torch.tensor.item()可以将只有一个元素的tensor转化为标量\n",
    "        return loss.item()\n",
    "\n",
    "    def similarity(self, token1: str, token2: str) -> float:\n",
    "        \"\"\" 计算两个词的相似性 \"\"\"\n",
    "        v1 = self.U_proj.weight.T[self.vocab.token_to_idx(token1)]\n",
    "        v2 = self.U_proj.weight.T[self.vocab.token_to_idx(token2)]\n",
    "        # 余弦相似度\n",
    "        return torch.cosine_similarity(v1, v2).item()\n",
    "\n",
    "    def most_similar_tokens(self, token: str, n: int):\n",
    "        \"\"\" 召回与token最相似的n个token \"\"\"\n",
    "        idx = self.vocab.token_to_idx(token, warn=True)\n",
    "        token_v = self.U_proj.weight.T[idx]\n",
    "\n",
    "        similarities = torch.cosine_similarity(token_v, self.U_proj.weight.T)\n",
    "        nbest_idx = similarities.argsort(descending=True)[:n] # srgsort降序排列，返回最大的前n个的索引\n",
    "\n",
    "        results = []\n",
    "        for idx in nbest_idx:\n",
    "            _token = self.vocab.idx_to_token(idx)\n",
    "            results.append((_token, similarities[idx].item()))\n",
    "\n",
    "        return results\n",
    "\n",
    "    def save_model(self, path: str):\n",
    "        \"\"\" 将模型保存到`path`路径下，如果不存在`path`会主动创建 \"\"\"\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        self.vocab.save_vocab(path)\n",
    "        torch.save(self.state_dict(), os.path.join(path, \"model.pth\"))\n",
    "\n",
    "    @classmethod\n",
    "    def load_model(cls, path: str):\n",
    "        \"\"\" 从`path`加载模型 \"\"\"\n",
    "        vocab = Vocab.load_vocab(path)\n",
    "        state_dict = torch.load(os.path.join(path, \"model.pth\"))\n",
    "        model = cls(vocab, state_dict[\"U_proj.weight\"].size(0))\n",
    "        model.load_state_dict(state_dict)\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试\n",
    "\n",
    "测试部分可用于验证CBOW实现的正确性。为了方便检查结果，请不要对训练的参数做修改。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试1：loss计算与反向传播\n",
    "\n",
    "本测试使用torch自带的损失函数与梯度反传功能对张量进行计算。如果你的实现正确，应当可以看到手动计算与自动计算得到的损失与梯度值相等或几近相等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总Token数: 9\n",
      "词表大小: 6\n",
      "限制大小后的词表总Token数: 10\n",
      "********** 参考值 **********\n",
      "Loss: 1.3631027936935425\n",
      "Gradient of U_proj:\n",
      "tensor([[ 0.0000,  0.0000,  0.0378,  0.0378,  0.0000,  0.0000],\n",
      "        [-0.0000, -0.0000, -0.2758, -0.2758, -0.0000, -0.0000],\n",
      "        [ 0.0000,  0.0000,  0.2454,  0.2454,  0.0000,  0.0000]])\n",
      "Gradient of V_proj:\n",
      "tensor([[-0.0123,  0.0084, -0.0820],\n",
      "        [ 0.0767, -0.0524,  0.5121],\n",
      "        [-0.0117,  0.0080, -0.0781],\n",
      "        [-0.0187,  0.0128, -0.1246],\n",
      "        [-0.0203,  0.0139, -0.1354],\n",
      "        [-0.0138,  0.0094, -0.0919]])\n",
      "\n",
      "********** 测试值 **********\n",
      "Loss: 1.3631027936935425\n",
      "Gradient of U_proj:\n",
      "tensor([[ 0.0000, -0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0000,  0.0000],\n",
      "        [ 0.0378, -0.2758,  0.2454],\n",
      "        [ 0.0378, -0.2758,  0.2454],\n",
      "        [ 0.0000, -0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0000,  0.0000]])\n",
      "Gradient of V_proj:\n",
      "tensor([[-0.0123,  0.0767, -0.0117, -0.0187, -0.0203, -0.0138],\n",
      "        [ 0.0084, -0.0524,  0.0080,  0.0128,  0.0139,  0.0094],\n",
      "        [-0.0820,  0.5121, -0.0781, -0.1246, -0.1354, -0.0919]])\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def test1():\n",
    "    random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    vocab = Vocab(corpus=\"./data/debug1.txt\")\n",
    "    cbow = CBOW(vocab, vector_dim=3)\n",
    "\n",
    "    print(\"********** 参考值 **********\")\n",
    "    x = F.one_hot(\n",
    "        torch.tensor([cbow.vocab.token_to_idx(\"1\"), cbow.vocab.token_to_idx(\"3\")]), num_classes=len(vocab)\n",
    "    ).float().sum(dim=0)\n",
    "    label = F.one_hot(torch.tensor(cbow.vocab.token_to_idx(\"2\")), num_classes=len(vocab)).float()\n",
    "    y, (h, o) = cbow(x)\n",
    "    loss_fct = torch.nn.CrossEntropyLoss()\n",
    "    loss = loss_fct(o.unsqueeze(0), torch.argmax(label).unsqueeze(0))\n",
    "    loss.backward()\n",
    "    print(\"Loss:\", loss.item())\n",
    "    print(f\"Gradient of U_proj:\\n{cbow.U_proj.weight.grad}\")\n",
    "    print(f\"Gradient of V_proj:\\n{cbow.V_proj.weight.grad}\")\n",
    "\n",
    "    print(\"\\n********** 测试值 **********\")\n",
    "    cbow.train_one_step([\"1\", \"3\"], \"2\", 1, debug=True)\n",
    "    \n",
    "test1()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试2：CBOW的简单训练\n",
    "\n",
    "本测试可用于验证CBOW的整个训练流程。如果你的实现正确，可以看到最终一个epoch的平均loss约在0.5~0.6，并且“i”、“he”和“she”的相似性较高。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总Token数: 50\n",
      "词表大小: 21\n",
      "限制大小后的词表总Token数: 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 50/50 [00:00<00:00, 2238.97it/s, Avg. loss=2.8967]\n",
      "Epoch 2: 100%|██████████| 50/50 [00:00<00:00, 1790.36it/s, Avg. loss=1.7551]\n",
      "Epoch 3: 100%|██████████| 50/50 [00:00<00:00, 1946.37it/s, Avg. loss=1.2534]\n",
      "Epoch 4: 100%|██████████| 50/50 [00:00<00:00, 2074.99it/s, Avg. loss=0.8772]\n",
      "Epoch 5: 100%|██████████| 50/50 [00:00<00:00, 2770.82it/s, Avg. loss=0.7364]\n",
      "Epoch 6: 100%|██████████| 50/50 [00:00<00:00, 2552.12it/s, Avg. loss=0.7606]\n",
      "Epoch 7: 100%|██████████| 50/50 [00:00<00:00, 2258.01it/s, Avg. loss=0.5265]\n",
      "Epoch 8: 100%|██████████| 50/50 [00:00<00:00, 2278.18it/s, Avg. loss=0.5395]\n",
      "Epoch 9: 100%|██████████| 50/50 [00:00<00:00, 3176.78it/s, Avg. loss=0.5325]\n",
      "Epoch 10: 100%|██████████| 50/50 [00:00<00:00, 2855.72it/s, Avg. loss=0.5058]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总耗时 0.24s\n",
      "[('i', 1.0), ('he', 0.9992085695266724), ('she', 0.9746933579444885), ('will', 0.7005326151847839), ('home', 0.3535130023956299)]\n",
      "[('he', 1.0), ('i', 0.9992084503173828), ('she', 0.9763324856758118), ('will', 0.7088274955749512), ('home', 0.37005138397216797)]\n",
      "[('she', 1.0), ('he', 0.9763324856758118), ('i', 0.9746933579444885), ('will', 0.6749593019485474), ('home', 0.37109506130218506)]\n"
     ]
    }
   ],
   "source": [
    "def test2():\n",
    "    random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    vocab = Vocab(corpus=\"./data/debug2.txt\")\n",
    "    cbow = CBOW(vocab, vector_dim=8)\n",
    "    cbow.train(corpus=\"./data/debug2.txt\", window_size=3, train_epoch=10, learning_rate=1.0)\n",
    "\n",
    "    print(cbow.most_similar_tokens(\"i\", 5))\n",
    "    print(cbow.most_similar_tokens(\"he\", 5))\n",
    "    print(cbow.most_similar_tokens(\"she\", 5))\n",
    "\n",
    "test2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试3：正式训练CBOW模型\n",
    "\n",
    "本测试将会在`treebank.txt`上训练词向量，为了加快训练流程，实验只保留高频的4000词，且词向量维度为20。\n",
    "\n",
    "在每个epoch结束后，会在`data/treebank.txt`中测试词向量的召回能力。如下所示，`data/treebank.txt`中每个样例为`word`以及对应的同义词，同义词从wordnet中获取。\n",
    "\n",
    "```python\n",
    "[\n",
    "    \"about\",\n",
    "    [\n",
    "        \"most\",\n",
    "        \"virtually\",\n",
    "        \"around\",\n",
    "        \"almost\",\n",
    "        \"near\",\n",
    "        \"nearly\",\n",
    "        \"some\"\n",
    "    ]\n",
    "]\n",
    "```\n",
    "\n",
    "> 本阶段预计消耗40分钟，具体时间与代码实现有关。最后一个epoch平均loss降至5.1左右，并且在同义词上的召回率约为17~18%左右。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总Token数: 205068\n",
      "词表大小: 4000\n",
      "限制大小后的词表总Token数: 179959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 205058/205058 [03:23<00:00, 1005.72it/s, Avg. loss=5.9898]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall rate: 6.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 205058/205058 [03:30<00:00, 976.20it/s, Avg. loss=5.5924] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall rate: 10.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 205058/205058 [03:19<00:00, 1027.00it/s, Avg. loss=5.4402]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall rate: 14.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 205058/205058 [03:14<00:00, 1056.26it/s, Avg. loss=5.3380]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall rate: 15.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 205058/205058 [03:17<00:00, 1038.68it/s, Avg. loss=5.2612]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall rate: 15.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 205058/205058 [03:27<00:00, 989.80it/s, Avg. loss=5.2002] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall rate: 15.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 205058/205058 [03:23<00:00, 1007.01it/s, Avg. loss=5.1504]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall rate: 16.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 205058/205058 [03:18<00:00, 1035.14it/s, Avg. loss=5.1089]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall rate: 16.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 205058/205058 [03:20<00:00, 1021.58it/s, Avg. loss=5.0738]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall rate: 17.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 205058/205058 [03:28<00:00, 983.53it/s, Avg. loss=5.0437] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall rate: 18.05%\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def calculate_recall_rate(model: CBOW, word_synonyms: List[Tuple[str, List[str]]], topn: int) -> float:\n",
    "    \"\"\" 测试CBOW的召回率 \"\"\"\n",
    "    hit, total = 0, 1e-9\n",
    "    for word, synonyms in word_synonyms:\n",
    "        synonyms = set(synonyms)\n",
    "        recalled = set([w for w, _ in model.most_similar_tokens(word, topn)])\n",
    "        hit += len(synonyms & recalled)\n",
    "        total += len(synonyms)\n",
    "\n",
    "    print(f\"Recall rate: {hit / total:.2%}\")\n",
    "    return hit / total\n",
    "\n",
    "def test3():\n",
    "    random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    corpus = \"./data/treebank.txt\"\n",
    "    lr = 1e-1\n",
    "    topn = 40\n",
    "\n",
    "    vocab = Vocab(corpus, max_vocab_size=4000)\n",
    "    model = CBOW(vocab, vector_dim=20)\n",
    "\n",
    "    dataset = CorpusDataset(corpus, window_size=4)\n",
    "\n",
    "    with open(\"data/synonyms.json\", encoding=\"utf-8\") as f:\n",
    "        word_synonyms: List[Tuple[str, List[str]]] = json.load(f)\n",
    "\n",
    "    for epoch in range(1, 11):\n",
    "        model.train_one_epoch(epoch, dataset, learning_rate=lr)\n",
    "        calculate_recall_rate(model, word_synonyms, topn)\n",
    "\n",
    "test3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验总结\n",
    "\n",
    "> [8] TODO：请在这里写下你的实验总结。**【1分】**\n",
    "\n",
    "### 实验总结\n",
    "\n",
    "在本次实验中，我详细阅读了代码，并在阅读中对每一部分进行了理解注释，对于Word2Vec词袋的原理和实现都有了更深的理解。\n",
    "\n",
    "在代码实现中，有两个需要格外注意的点。\n",
    "1. 关于softmax函数，最开始我只是简单的按照定义 exp(x_i)/Σ_j(exp(x_j)) 进行计算，虽然在测试样例中都可以通过，但是进行训练时就会在结果中出现NAN，分析来说是因为数值溢出，解决思路就是在计算softmax之前进行归一化，实现方式是计算是分子分母同时除以分布最大值，这样就不会导致分子或者分母数值溢出，即公式 exp(x_i-max(x))/Σ_j(exp(x_j)-max(x))。但是修改后发现仍然存在NAN，这就引出了第二个需要注意的点。\n",
    "2. 在前向计算中不归一化也会导致梯度爆炸。因为输入的x独热编码，实际上是标记了许多个词，一般来说sum(x)>1，经过U投影到隐层，再经由V投影回，反复叠加计算会导致梯度爆炸，所以需要进行归一化，经尝试在隐层或者输入归一化都可以，只是要注意计算梯度时的系数，归一化操作在forward函数中进行。\n",
    "\n",
    "另外，由于在计算梯度时，我得到的结果就是梯度形状，但是给出的打印代码有转置，所以输出上形状不同。\n",
    "\n",
    "在使用torch以及自己写梯度更新时的其他发现。\n",
    "1. 对于权重更新方式，起初我使用了weight.data的原地更新方式（代码中可见），但是原地操作直接修改现有张量的数据，可能会影响计算图的构建，导致错误。所以后面改用torch.nn.Parameter进行参数更新，既可以确保计算图的完整性，也可以提高代码可读性，清楚表明新参数的创建和注册。\n",
    "2. 进行了上述debug后，却发现一个epoch竟然需要一个小时之久！我苦苦思考，最终在调整下述代码时，训练速度发生了质的飞跃，单个epoch速度快了20倍！<br>\n",
    "x = x/sum(x) 调整为 x = x/torch.sum(x)<br>\n",
    "查询torch.sum和python内置sum作用于tensor张量的区别，了解到在处理大规模数据和高维张量时，torch.sum的性能通常比Python内置的sum函数更高，因为python内置的sum函数会将输入看作一个可迭代对象，逐元素进行求和，相当于过了一个for循环，而torch.sum利用了PyTorch的底层优化和硬件加速，可以并行计算、减少数据传输开销等。使用GPT辅助，创建一个10^6大张量进行性能测试，发现torch.sum比sum快了十倍。\n",
    "\n",
    "总结来说，所有的测试都取得了和标准样例一样的结果，在正式训练测试test3中训练10个epoch之后得到的损失函数值是5.0437，同义词召回率是18.05%，均符合预期。另外，速度比预计的更快，用时约33分钟。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
